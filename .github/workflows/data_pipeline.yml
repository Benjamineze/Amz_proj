name: Data Pipeline Automation

on:
  schedule:
    - cron: '0 22 * * MON'  # Run every Tuesday at midnight
  workflow_dispatch:  # Option to run manually

jobs:
  scrape_and_clean:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run scraping script
        run: |
          python scripts/scraper.py  # scraping script

      - name: Run data cleaning and append script
        run: |
          python scripts/cleaner.py  # Your combined cleaning and appending script

      - name: Set up Google Cloud credentials  # Fixed indentation here
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GOOGLE_CLOUD_CREDENTIALS }}  # Secret here
        run: |
          echo "${{ secrets.GOOGLE_CLOUD_CREDENTIALS }}" > google_cloud_credentials.json  # Writing the secret to the file
          export GOOGLE_APPLICATION_CREDENTIALS=google_cloud_credentials.json  # Setting environment variable

      - name: Authenticate to Google Cloud
        run: |
          gcloud auth activate-service-account --key-file $HOME/gcloud-key.json
          gcloud config set project amaz-project-438116
